import streamlit as st
import pandas as pd
from PIL import Image
import os, ast
import subprocess
import time

st.set_page_config(layout="wide")

st.title("MultiModal Recommendation for Text-to-Image Generation")

gen_img_metadata = "image_generated/gen_img_metadata.csv"
# Generated image metadata column names: ['model', 'image_key', 'prompt', 'gen_img_path']

image_dir = "image_generated"
  
ground_truth_images = "DeepFashion/images"

# check is CSV and images directory exist
if not os.path.exists(gen_img_metadata):
    st.error(f"{gen_img_metadata} not found.")
    st.stop()
    
# Ensure that "image_generated" folder exists
if not os.path.exists(image_dir):
    st.error(f"{image_dir} folder not found")
    st.stop()
    
# Load CSV
df = pd.read_csv(gen_img_metadata)

# create tabs:
tab1, tab2, tab3, tab4 = st.tabs([
    "Compare Images",
    "Evaluation Metrics",
    "System Design",
    "Disclaimer"
])

# --------------------------------------------------- Tab: Compare Image  --------------------------------------------------- 
with tab1:        
    # Extract Unique prompts from the CSV
    prompts = df["prompt"].unique()

    # dropdown to select which prompt to display
    selected_prompt = st.selectbox("Select a prompt:", prompts)
    
    filtered_df = df[df["prompt"] == selected_prompt]
    if not filtered_df.empty:
      gt_filename = filtered_df.iloc[0]["image_key"]
      prompt_text = selected_prompt
    else:
      st.error("No data found for the selected prompt")
      st.stop()
    
    
    # show ground-truth image
    gt_path = os.path.join(ground_truth_images, gt_filename)
    
    # Filter DataFrame by the chosen prompt
    filtered_df = df[df["prompt"] == selected_prompt]

    # For each model in the filtered DataFrame, vertical display the image + metrics
    models_for_prompt = filtered_df["model"].unique()
    
    # Resize function to unify image sizes
    def resize_image(img, size=(768, 1024)):
      return img.resize(size)
    
    # gather all images (ground truth + generated) into a list
    images_to_display = []
    
    # ground truth first
    if os.path.exists(gt_path):
      gt_img = Image.open(gt_path).convert("RGB")
      gt_img = resize_image(gt_img)
      images_to_display.append(("Ground Truth from DeepFashion Dataset", gt_img))
    else:
      st.warning(f"Ground Truth image not found: {gt_filename}")
      
    # then each model's generated image
    for model_name in models_for_prompt:
      row = filtered_df[filtered_df["model"]==model_name]
      if row.empty:
        continue
      row_data = row.iloc[0]
      image_path = row_data["gen_img_path"]
      if os.path.exists(image_path):
        gen_img = Image.open(image_path).convert("RGB")
        gen_img = resize_image(gen_img)
        images_to_display.append((model_name, gen_img))
        
      
    # display images in a grid
    cols_per_row = 3
    rows_needed = (len(images_to_display) + cols_per_row -1 )//cols_per_row
    
    index = 0
    for _ in range(rows_needed):
      cols = st.columns(cols_per_row)
      for col_i in range(cols_per_row):
        if index < len(images_to_display):
          model_title, img_obj = images_to_display[index]
          with cols[col_i]:
                st.markdown(
                    f"<h3 style='font-size:24px; color:#2e5f9c;'>{model_title}</h3>",
                    unsafe_allow_html=True
                )
                st.image(img_obj, use_container_width=True)
          index += 1
           
    st.markdown(f"<p style='font-size:14px;'>{prompt_text}</p>", unsafe_allow_html=True)
    st.write("---")
    st.markdown("**End of Comparison**")

# --------------------------------------------------- Tab: "Evaluation Metrics"  --------------------------------------------------- 
with tab2:
    st.subheader("Evaluation Metrics - Per-Model Results")
    df1 = pd.read_csv("results/evaluation_results.csv")
    st.dataframe(df1)
    
    # display evaluation metrics comparison image from MLflow, stored at results/mlflow.png
    mlflow_image_path = "results/mlflow.png"
    
    st.subheader("Comaprison of Evaluation Metrics for Text-to-Image Generation Models")
    st.markdown("**Note:** The image below is generated by `MLflow` and saved in the `results` folder.")
    if os.path.exists(mlflow_image_path):
        st.image(mlflow_image_path, use_container_width=True)
    else:
        st.error(f"MLflow image not found: {mlflow_image_path}")

    st.markdown("---")
    # st.subheader("ðŸ“Š Evaluation Metric Descriptions")

    # --- Layman Explanation ---
    st.subheader("Evaluation Metric Descriptions")
    explanation = {
        "Metric": [
            "Avg_Clip_Score_Prompt_GenImg",
            "Avg_Clip_Cos_Sim_GenImg_GTimg",
            "Avg_LPIPS_GenImg_GTimg",
            "FID_Frechet_inception_distance",
            "MRR_Mean_Reciprocal_Rank",
            "Recall@3"
        ],
        "Meaning": [
            "How well the image matches the given text prompt",
            "How visually similar the generated image is to the real one",
            "Measures visible differencesâ€”lower means more realistic",
            "Quality score comparing real vs. generated imagesâ€”lower is better",
            "Measures how quickly the correct match appears when sorted by relevance",
            "Shows how often the correct match is in the top 3 results"
        ],
    "Technical Description": [
        "Uses CLIP ViT-B/32 model. Computes logits_per_image by passing (prompt, generated image) into CLIP and taking the image-text similarity score. Average over all samples.",
        "Generates CLIP embeddings for gen_img and GT_img using ViT-B/32. Computes cosine similarity between each pair and averages the similarity scores over the dataset.",
        "Uses LPIPS metric with a pre-trained VGG network. Extracts intermediate features and computes L2 distance between normalized features of gen and GT images. Lower means perceptually similar.",
        "Uses InceptionV3 activations to extract high-dimensional features. Computes FrÃ©chet Distance between generated and real image distributions using their means (Î¼) and covariances (Î£).",
        "Ranks GT image among all others based on cosine similarity between gen_img and GT pool embeddings. MRR = mean of 1 / rank_of_correct_GT, measuring average retrieval position.",
        "Similar to MRR but binary. For each gen_img, checks if the correct GT image is among top-3 most similar (cosine) images. Computes average success rate over all samples."
    ]
    }
    st.table(pd.DataFrame(explanation))
  
# # --------------------------------------------------- Tab: "Project Structure"  --------------------------------------------------- 
with tab3:
    st.subheader("System Architecture")
    st.image("README_files/Sys_design.png", use_container_width=True)

    st.subheader("Code Explanation")
    st.markdown("""
- **[evaluation_metrics.py](https://github.com/kapilw25/Evaluation_generated_images/blob/main/evaluation_metrics.py)**  
  Contains functions to calculate evaluation metrics for text-to-image outputs.
- **[evaluation_pipeline.py](https://github.com/kapilw25/Evaluation_generated_images/blob/main/evaluation_pipeline.py)**  
  Generates images using text-to-image models and computes evaluation metrics, saving results to CSV.
- **[evaluation_results.csv](https://github.com/kapilw25/Evaluation_generated_images/blob/main/results/evaluation_results.csv)**  
  CSV file that stores all computed evaluation metrics.
- **[visualization_app.py](https://github.com/kapilw25/Evaluation_generated_images/blob/main/visualization_app.py)**  
  Streamlit app that visualizes the generated images and evaluation metrics.
- **[image_generated/](https://github.com/kapilw25/Evaluation_generated_images/blob/main/image_generated)**  
  Directory containing the generated images.
    """, unsafe_allow_html=True)
    
    

# --------------------------------------------------- Tab: "Disclaimer"  --------------------------------------------------- 
with tab4:
    st.subheader("Disclaimer")
    st.markdown("""
- This app is for education, research and display purposes only. \n
- All images are generated via huggingface API. \n
- MultiModal evaluations are executed on local Nvidia machine [**CUDA Device: NVIDIA GeForce RTX 2080 SUPER**] with 8GB vRAM, provided by San Jose State University, CA.
    """)