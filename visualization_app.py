import streamlit as st
import pandas as pd
from PIL import Image
import os, ast
import subprocess
import time

st.set_page_config(layout="wide")

st.title("MultiModal Recommendation for Text-to-Image Generation")

# Load CSVs
gen_img_metadata = "image_generated/gen_img_metadata.csv"
df_gen_img_metadata = pd.read_csv(gen_img_metadata)
# Generated image metadata column names: ['model', 'image_key', 'prompt', 'gen_img_path']
df_evaluation_results = pd.read_csv("results/evaluation_results.csv")

image_dir = "image_generated"
  
ground_truth_images = "DeepFashion/images"

# check is CSV and images directory exist
if not os.path.exists(gen_img_metadata):
    st.error(f"{gen_img_metadata} not found.")
    st.stop()
    
# Ensure that "image_generated" folder exists
if not os.path.exists(image_dir):
    st.error(f"{image_dir} folder not found")
    st.stop()
    


# create tabs:
tab1, tab2, tab3, tab4 = st.tabs([
    "Compare Images",
    "Evaluation Metrics",
    "System Design",
    "Disclaimer"
])

# --------------------------------------------------- Tab: Compare Image  --------------------------------------------------- 
with tab1:        
    # Extract Unique prompts from the CSV
    prompts = df_gen_img_metadata["prompt"].unique()

    # dropdown to select which prompt to display
    selected_prompt = st.selectbox("Select a prompt:", prompts)
    
    # map the selected prompt to its corresponding image key
    selected_image_key = df_gen_img_metadata[df_gen_img_metadata["prompt"] == selected_prompt]["image_key"].iloc[0]
    prompt_text = selected_prompt
    
    # Filter Dataframe by the chosen image key so both model and model_metadata rows are included
    filtered_df = df_gen_img_metadata[df_gen_img_metadata["image_key"] == selected_image_key]
    if filtered_df.empty:
      st.error("No data found for the selected image key")
      st.stop()
    
    # show ground-truth image
    gt_path = os.path.join(ground_truth_images, selected_image_key)
    
    # Filter DataFrame by selected image key to get all model variants
    filtered_df = df_gen_img_metadata[df_gen_img_metadata["image_key"] == selected_image_key]

    # For each model in the filtered DataFrame, vertical display the image + metrics
    models_for_prompt = filtered_df["model"].unique()
    # print(f"models_for_prompt : {models_for_prompt}")
    # Sort models by their weighted score
    model_scores = df_evaluation_results[df_evaluation_results["Model"].isin(models_for_prompt)][["Model", "Weighted Score ‚¨ÜÔ∏è"]]
    model_scores = model_scores.sort_values("Weighted Score ‚¨ÜÔ∏è", ascending=False)
    sorted_models = model_scores["Model"].tolist()

    # Resize function to unify image sizes
    def resize_image(img, size=(768//2, 1024//2)):
      return img.resize(size)
    
    # Retrieve and resize ground truth image once
    if os.path.exists(gt_path):
      gt_img = Image.open(gt_path).convert("RGB")
      gt_img = resize_image(gt_img)
    else:
      st.warning(f"Ground Truth image not found: {selected_image_key}")
      
    # Ground model images into pairs by base model name (e.g - "Flux", "CogView", etc.)
    ordered_bases = []
    base_model_map = {}
    for model in sorted_models:
      base = model.replace('_Metadata', '')
      if base not in ordered_bases:
        ordered_bases.append(base)
      if base not in base_model_map:
        base_model_map[base] = {}
      if model.endswith("_Metadata"):
        base_model_map[base]['metadata'] = model
      else:
        base_model_map[base]['base'] = model
        
    # for each base model, display a row with three columns:
    # [ground truth image >> image generated by base model >> image generated by metadata variant]
    for base in ordered_bases:
      base_img, meta_img = None, None
      
      # get and resize the base model image
      if 'base' in base_model_map[base]:
        base_row = filtered_df[filtered_df["model"] == base_model_map[base]['base']]
        if not base_row.empty:
          image_path = base_row.iloc[0]["gen_img_path"]
          base_prompt = base_row.iloc[0]["prompt"] # capture base model prompt
          if os.path.exists(image_path):
            base_img = resize_image(Image.open(image_path).convert("RGB"))
            
      # get and resize the metadata variant image
      if 'metadata' in base_model_map[base]:
        meta_row = filtered_df[filtered_df["model"] == base_model_map[base]['metadata']]
        if not meta_row.empty:
          image_path = meta_row.iloc[0]["gen_img_path"]
          meta_prompt = meta_row.iloc[0]["prompt"] # capture metadata variant prompt
          if os.path.exists(image_path):
            meta_img = resize_image(Image.open(image_path).convert("RGB"))
            
      # Display the row with three columns
      cols = st.columns(3)
      with cols[0]:
          st.markdown("<h3 style='font-size:24px; color:#2e5f9c;'>Ground Truth</h3>", unsafe_allow_html=True)
          st.image(gt_img, use_container_width=True)
      with cols[1]:
          st.markdown(f"<h3 style='font-size:24px; color:#2e5f9c;'>{base}</h3>", unsafe_allow_html=True)
          if base_img:
              st.image(base_img, use_container_width=True)
              st.markdown(f"<p style='font-size:12px;'>{base_prompt}</p>", unsafe_allow_html=True)
      with cols[2]:
          st.markdown(f"<h3 style='font-size:24px; color:#2e5f9c;'>{base}_Metadata</h3>", unsafe_allow_html=True)
          if meta_img:
              st.image(meta_img, use_container_width=True)
              st.markdown(f"<p style='font-size:10px;'>{meta_prompt}</p>", unsafe_allow_html=True)
      
      
    # st.markdown(f"<p style='font-size:14px;'>{prompt_text}</p>", unsafe_allow_html=True)
    st.write("---")
    st.markdown("**End of Comparison**")

# --------------------------------------------------- Tab: "Evaluation Metrics"  --------------------------------------------------- 
with tab2:
    st.subheader("Evaluation Metrics - Per-Model Results")
    # wrap text in the table to make sure it fits the screen
    st.markdown(
        """
        <style>
            .wrapped-text {
                white-space: normal;
            }
        </style>
        """,
        unsafe_allow_html=True
    )
    # display each numerical value upto 2 decimal points
    st.table(
        df_evaluation_results.style
          .format(lambda x: "{:.2f}".format(x) if isinstance(x, (int, float)) else x)
          .set_table_attributes('class="wrapped-text"')
    )


    
    st.markdown("---")
    
    # display evaluation metrics comparison image from MLflow, stored at results/mlflow.png
    mlflow_image_path = "results/mlflow.png"
    
    st.subheader("Comaprison of Evaluation Metrics for Text-to-Image Generation Models")
    st.markdown("**Note:** The image below is generated by `MLflow` and saved in the `results` folder.")
    if os.path.exists(mlflow_image_path):
        st.image(mlflow_image_path, use_container_width=True)
    else:
        st.error(f"MLflow image not found: {mlflow_image_path}")

    st.markdown("---")
    # st.subheader("üìä Evaluation Metric Descriptions")

    # --- Layman Explanation ---
    st.subheader("Evaluation Metric Descriptions")
    explanation = {
        "Metric": [
            "Weighted Score ‚¨ÜÔ∏è",
            "Avg Clip Score ‚¨ÜÔ∏è [Prompt vs GenIm]",
            "Avg Clip Cos Sim ‚¨ÜÔ∏è [GenImg vs GTimg]",
            "Avg LPIPS ‚¨áÔ∏è [GenImg vs GTimg]",
            "FID ‚¨áÔ∏è (Frechet inception distance)",
            "MRR ‚¨ÜÔ∏è (Mean Reciprocal Rank)",
            "Recall@3 ‚¨ÜÔ∏è"
        ],
        "Meaning": [
            "A composite score that combines multiple metrics to provide an overall evaluation of the model's performance. Higher is better.",
            "Measures how well the generated image aligns with the input prompt. A higher score indicates that the image better captures the semantic content of the prompt",
            "Evaluates the similarity between the generated image and the ground truth using CLIP embeddings. Higher values mean closer similarity",
            "Focuses on perceptual similarity; a lower LPIPS score indicates that the generated image is perceptually closer to the ground truth",
            "Measures the distributional difference between generated and real images. A lower FID score signifies that the generated images are closer in distribution to the ground truth images",
            "Assess how effectively the generated image can ‚Äúretrieve‚Äù its corresponding ground truth image. A higher MRR indicates that the correct match is ranked higher in the list of similar images",
            "Shows how often the correct match is in the top 3 results. A higher recall rate indicates better retrieval performance"
        ],
    "Technical Description": [
        "Computes a composite score as: 0.4 √ó (Normalized CLIP Cosine) + 0.3 √ó (Normalized LPIPS) + 0.15 √ó (Normalized FID) + 0.1 √ó (Normalized Retrieval) + 0.05 √ó (Normalized CLIP Score). Normalization is performed via min‚Äìmax scaling, with inversion for metrics where lower is better.",
        "Uses CLIP ViT-B/32 model. Computes logits_per_image by passing (prompt, generated image) into CLIP and taking the image-text similarity score. Average over all samples.",
        "Generates CLIP embeddings for gen_img and GT_img using ViT-B/32. Computes cosine similarity between each pair and averages the similarity scores over the dataset.",
        "Uses LPIPS metric with a pre-trained VGG network. Extracts intermediate features and computes L2 distance between normalized features of gen and GT images. Lower means perceptually similar.",
        "Uses InceptionV3 activations to extract high-dimensional features. Computes Fr√©chet Distance between generated and real image distributions using their means (Œº) and covariances (Œ£).",
        "Ranks GT image among all others based on cosine similarity between gen_img and GT pool embeddings. MRR = mean of 1 / rank_of_correct_GT, measuring average retrieval position.",
        "Similar to MRR but binary. For each gen_img, checks if the correct GT image is among top-3 most similar (cosine) images. Computes average success rate over all samples."
    ]
    }
    st.table(pd.DataFrame(explanation))
  
# # --------------------------------------------------- Tab: "Project Structure"  --------------------------------------------------- 
with tab3:
    st.subheader("System Architecture")
    st.image("README_files/Sys_design.png", use_container_width=True)

    st.subheader("Code Explanation")
    st.markdown("""
- **[evaluation_metrics.py](https://github.com/kapilw25/Evaluation_generated_images/blob/main/evaluation_metrics.py)**  
  Contains functions to calculate evaluation metrics for text-to-image outputs.
- **[evaluation_pipeline.py](https://github.com/kapilw25/Evaluation_generated_images/blob/main/evaluation_pipeline.py)**  
  Generates images using text-to-image models and computes evaluation metrics, saving results to CSV.
- **[evaluation_results.csv](https://github.com/kapilw25/Evaluation_generated_images/blob/main/results/evaluation_results.csv)**  
  CSV file that stores all computed evaluation metrics.
- **[visualization_app.py](https://github.com/kapilw25/Evaluation_generated_images/blob/main/visualization_app.py)**  
  Streamlit app that visualizes the generated images and evaluation metrics.
- **[image_generated/](https://github.com/kapilw25/Evaluation_generated_images/blob/main/image_generated)**  
  Directory containing the generated images.
    """, unsafe_allow_html=True)
    
# --------------------------------------------------- Tab: "Disclaimer"  --------------------------------------------------- 
with tab4:
    st.subheader("Disclaimer")
    st.markdown("""
- This app is for education, research and display purposes only. \n
- All images are generated via huggingface API. \n
- MultiModal evaluations are executed on local Nvidia machine [**CUDA Device: NVIDIA GeForce RTX 2080 SUPER**] with 8GB vRAM, provided by San Jose State University, CA.
    """)